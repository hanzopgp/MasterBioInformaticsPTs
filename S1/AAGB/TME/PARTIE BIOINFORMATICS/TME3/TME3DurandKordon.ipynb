{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karna\\anaconda3\\lib\\site-packages\\ete3-3.1.2-py3.7.egg\\ete3\\evol\\parser\\codemlparser.py:221: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "C:\\Users\\karna\\anaconda3\\lib\\site-packages\\ete3-3.1.2-py3.7.egg\\ete3\\evol\\parser\\codemlparser.py:221: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from ete3 import Tree\n",
    "import pprint\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "STEP_MATRIX = np.array([[0, 3, 4, 9],\n",
    "                       [3, 0, 2, 4],\n",
    "                       [4, 2, 0, 4],\n",
    "                       [9, 4, 4, 0]])\n",
    "\n",
    "N_SPECIES = STEP_MATRIX.shape[0]\n",
    "\n",
    "STR_JOIN = \"+\"\n",
    "\n",
    "CHAR_ORDER = [\"A\", \"C\", \"G\", \"T\"]\n",
    "\n",
    "DIC1 = {\n",
    "          'Probopass':np.array([0,np.inf,np.inf,np.inf]),\n",
    "          'Aggron':np.array([np.inf,np.inf,np.inf,0]),\n",
    "          'Bastiodon':np.array([np.inf,np.inf,np.inf,0]),\n",
    "          'Regirock':np.array([np.inf,np.inf,0,np.inf]),\n",
    "          'Registeel':np.array([np.inf,np.inf,0,np.inf]),\n",
    "          'Regice':np.array([np.inf,np.inf,0,np.inf]),\n",
    "          'Klingklang':np.array([np.inf,np.inf,0,np.inf]),\n",
    "          'Metagross':np.array([np.inf,0,np.inf,np.inf]),\n",
    "          'Genesect':np.array([0,np.inf,np.inf,np.inf]),\n",
    "          'Porygon=Z':np.array([np.inf,0,np.inf,np.inf]),\n",
    "          'Magnezone':np.array([np.inf,0,np.inf,np.inf]),\n",
    "          'Forretress':np.array([np.inf,np.inf,np.inf,0]),\n",
    "          'Electrode':np.array([0,np.inf,np.inf,np.inf]),\n",
    "          'Ferrothorn':np.array([np.inf,np.inf,0,np.inf]),\n",
    "       }\n",
    "\n",
    "N1 = \"(((( Electrode , Magnezone) ,Porygon=Z) , (((( Aggron , Bastiodon ) , Forretress ) , Ferrothorn ) , ((((( Regirock , Regice ) , Registeel ) , Metagross ) , Klingklang ) , Genesect ))) , Probopass );\"\n",
    "N2 = \"((((( Regirock , Regice ) , Registeel ) , (( Metagross , Klingklang ) , Genesect )) , ((( Aggron , Bastiodon ) ,( Forretress , Ferrothorn )) , Probopass )) ,( Porygon=Z,( Magnezone , Electrode )));\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"tree1.png\" width=\"500px\">\n",
    "<img src=\"tree2.png\" width=\"500px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sankoff algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(((( Electrode , Magnezone) ,Porygon=Z) , (((( Aggron , Bastiodon ) , Forretress ) , Ferrothorn ) , ((((( Regirock , Regice ) , Registeel ) , Metagross ) , Klingklang ) , Genesect ))) , Probopass );\n",
      "\n",
      "            /-Electrode\n",
      "         /-|\n",
      "      /-|   \\-Magnezone\n",
      "     |  |\n",
      "     |   \\-Porygon=Z\n",
      "     |\n",
      "     |            /-Aggron\n",
      "     |         /-|\n",
      "     |      /-|   \\-Bastiodon\n",
      "   /-|     |  |\n",
      "  |  |   /-|   \\-Forretress\n",
      "  |  |  |  |\n",
      "  |  |  |   \\-Ferrothorn\n",
      "  |  |  |\n",
      "  |  |  |               /-Regirock\n",
      "  |  |  |            /-|\n",
      "  |   \\-|         /-|   \\-Regice\n",
      "--|     |        |  |\n",
      "  |     |      /-|   \\-Registeel\n",
      "  |     |     |  |\n",
      "  |     |   /-|   \\-Metagross\n",
      "  |     |  |  |\n",
      "  |      \\-|   \\-Klingklang\n",
      "  |        |\n",
      "  |         \\-Genesect\n",
      "  |\n",
      "   \\-Probopass\n"
     ]
    }
   ],
   "source": [
    "tree1 = Tree(N1)\n",
    "print(N1)\n",
    "print(tree1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sankoff(tree):\n",
    "    new_tree = parse_string(tree) # In this part we clean and parse the string to get an array\n",
    "    merge_cluster(new_tree) # In this part we merge the clusters and add the new array values of intern nodes in the dictionnary\n",
    "    init_solo_leaves() # Initialisation of the solo leaves for the traceback function\n",
    "    new_chars = traceback() # Traceback in the dictionnary and gets the new characters of the nodes\n",
    "    parcimony_score = compute_score(new_chars) # Computes the parcimony score of the tree\n",
    "    return parcimony_score\n",
    "\n",
    "def parse_string(tree):\n",
    "    tree = tree.replace(\" \",\"\")\n",
    "    tree = tree.replace(\";\",\"\")\n",
    "    new_tree = []\n",
    "    for i in range(len(tree)):\n",
    "        if tree[i] == \")\":\n",
    "            cpt = 0\n",
    "            for j in range(i, 0, -2):\n",
    "                if tree[j] == \")\" and tree[j-1] == \")\": # Discriminate mini/big cluster and get offset\n",
    "                    cpt += 1\n",
    "            tmp_tree = tree[:i] # Cut after parenthesis\n",
    "            n_close_par = tmp_tree.count(\")\") # Counts open and close parenthesis\n",
    "            n_open_par = tmp_tree.count(\"(\")\n",
    "            if cpt == 1: # Big cluster\n",
    "                parenthesis_to_cut = n_open_par - n_close_par + cpt + 1 # FIX BUG HERE WITH N2\n",
    "                nth = find_nth(tmp_tree, \"(\", parenthesis_to_cut) # Finds the index to cut at the right open parenthesis\n",
    "                tmp_tree = tmp_tree[nth:] \n",
    "            else: # Mini cluster\n",
    "                index_par = tmp_tree.rfind(\"(\") # For mini cluster : just cut at first open parenthesis met\n",
    "                tmp_tree = tmp_tree[index_par:]\n",
    "            tmp_tree = tmp_tree.replace(\"(\",\"\") # Delete all parenthesis to extract leaves\n",
    "            tmp_tree = tmp_tree.replace(\")\",\"\")\n",
    "            leaves = tmp_tree.split(\",\")\n",
    "            new_tree.append(leaves) # We build a new list of list containing leaves\n",
    "    return new_tree\n",
    "\n",
    "def merge_cluster(new_tree):\n",
    "    visited = []\n",
    "#     species = np.unique(np.concatenate(np.array(new_tree)).flatten()) # Gets the species of the tree\n",
    "    for i in range(len(new_tree)):\n",
    "        if len(new_tree[i]) == 2: # Merge of 2 elements\n",
    "            elt1 = new_tree[i][0]\n",
    "            elt2 = new_tree[i][1]\n",
    "#             print(\"we add\", elt1, \"and\", elt2)\n",
    "            new_ancester = add_ancester(elt1, elt2)\n",
    "            visited.append(new_ancester)\n",
    "        else: # Merge of clusters\n",
    "            tmp_str = new_tree[i][0]\n",
    "            str_possible = []\n",
    "            for j in range(1, len(new_tree[i])-1):\n",
    "                tmp_str += STR_JOIN + new_tree[i][j]\n",
    "                str_possible.append(tmp_str)\n",
    "            str_possible.reverse() # All the existing clusters in this array\n",
    "            if tmp_str in visited: # Merge of mini clusters\n",
    "                visited.remove(tmp_str) # If we find one we remove it from the visited array\n",
    "                del_elt = tmp_str.split(STR_JOIN)\n",
    "                new_elt = new_tree[i].copy() # Copy because we remove from new_elt after that\n",
    "                for elt in del_elt:\n",
    "                    new_elt.remove(elt)\n",
    "#                 print(\"we add\", tmp_str, \"and\", new_elt[0])\n",
    "                new_ancester = add_ancester(tmp_str, new_elt[0])\n",
    "                visited.append(new_ancester)\n",
    "            else: # Merge of big clusters\n",
    "                str_possible = []\n",
    "                for j in range(1, len(new_tree[i])):\n",
    "                    tmp_str += STR_JOIN + new_tree[i][j]\n",
    "                    str_possible.append(tmp_str)\n",
    "                str_possible.reverse() \n",
    "                new_list = []\n",
    "                for j in range(len(visited)):\n",
    "                    if visited[j] in tmp_str: # This time there can be multiple clusters\n",
    "                        new_list.append(visited[j])\n",
    "#                 print(\"we add\", new_list[0], \"and\", new_list[1])\n",
    "                new_ancester = add_ancester(new_list[0], new_list[1])\n",
    "                visited.append(new_ancester)\n",
    "                visited.remove(new_list[0])\n",
    "                visited.remove(new_list[1])\n",
    "    \n",
    "def find_nth(haystack, needle, n):\n",
    "    start = haystack.find(needle)\n",
    "    while start >= 0 and n > 1:\n",
    "        start = haystack.find(needle, start+len(needle))\n",
    "        n -= 1\n",
    "    return start\n",
    "    \n",
    "def add_ancester(elt1, elt2):\n",
    "    new_ancester = elt1+STR_JOIN+elt2 \n",
    "    if len(DIC1[elt1]) != 4: # Have to check if we just have the basic array\n",
    "        val1 = DIC1[elt1][0] # or if we have the array + backtrack characters\n",
    "    else:\n",
    "        val1 = DIC1[elt1]\n",
    "    if len(DIC1[elt2]) != 4:\n",
    "        val2 = DIC1[elt2][0]\n",
    "    else:\n",
    "        val2 = DIC1[elt2]\n",
    "    new_values = compute_new_values(val1, val2)\n",
    "    add_to_backtrack_dic(val1, val2, new_ancester, new_values)\n",
    "    return new_ancester\n",
    "    \n",
    "def compute_new_values(val1, val2):\n",
    "    new_array = np.zeros((N_SPECIES))\n",
    "    for i in range(N_SPECIES):\n",
    "        new_array[i] = np.min(STEP_MATRIX[i]+val1) + np.min(STEP_MATRIX[i]+val2)\n",
    "    return new_array\n",
    "\n",
    "def add_to_backtrack_dic(val1, val2, new_ancester, new_values):\n",
    "    min_index1, min_index2 = np.argmin(val1), np.argmin(val2)\n",
    "    DIC1[new_ancester] = [new_values, CHAR_ORDER[min_index1], CHAR_ORDER[min_index2]] \n",
    "    \n",
    "def init_solo_leaves():\n",
    "    for key in DIC1:\n",
    "        if STR_JOIN not in key:\n",
    "            DIC1[key] = [DIC1[key], CHAR_ORDER[np.argmin(DIC1[key])]]\n",
    "    \n",
    "def traceback():\n",
    "    new_chars = []\n",
    "    first_elem = True\n",
    "    for item in reversed(DIC1.values()):\n",
    "        tmp_item = item[1:] # Ignoring array values\n",
    "        if first_elem:\n",
    "            new_chars.append(random.choice(tmp_item))\n",
    "            first_elem = False\n",
    "        else:\n",
    "            if item in save_item:\n",
    "                for item in tmp_item:\n",
    "                    new_chars.append(item)\n",
    "            else:\n",
    "                new_chars.append(random.choice(tmp_item))\n",
    "        save_item = tmp_item\n",
    "    return new_chars\n",
    "    \n",
    "def compute_score(new_chars):\n",
    "    score = 0\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "parcimony_score = sankoff(N1)\n",
    "print(parcimony_score)\n",
    "# pprint.pprint(DIC1, sort_dicts=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
