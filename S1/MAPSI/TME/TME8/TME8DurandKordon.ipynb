{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ei9rT_XZ3SO5"
   },
   "source": [
    "### TME sur Echantillonage\n",
    "\n",
    "## Diffusion dans les graphes \n",
    "\n",
    "Au cours des vingt dernières années, les réseaux sociaux sont devenus un média d’information incontournable, mettant en jeu des dynamiques complexes de communication entre utilisateurs. La modélisation de la diffusion d’information sur les réseaux constitue depuis lors un enjeu majeur, pour diverses tâches\n",
    "telles que l’identification de leaders d’opinions, la prédiction ou la maximisation de l’impact d’un contenu diffusé, la détection de communautés d’opinions, ou plus généralement l’analyse des dynamiques du réseau considéré.\n",
    "\n",
    "Le modèle proposé par (Saito et al, 2009) considère une diffusion en cascade dans laquelle l'information transite de noeuds en noeuds du réseau en suivant des relations d'influence entre les utilisateurs. Lorsqu'un utilisateur est ``infecté'' par une information, il possède une chance unique de la retransmettre à chacun de ses successeurs dans le graphe, selon une probabilité définie sur le lien correspondant. Le modèle définit en fait deux paramètres sur chaque lien $(u,v)$ du graphe:\n",
    "\n",
    "\n",
    "*   $k_{u,v}$: la probabilité que l'utilisateur $u$ transmette une information diffusée à $v$\n",
    "*   $r_{u,v}$: si la transmission s'effectue, l'utilisateur $v$ la reçoit au temps $t_v=t_u+\\delta$, avec $\\delta \\sim Exp(r_{u,v})$\n",
    "\n",
    "Pour utiliser ce modèle, on devra donc échantillonner selon la distribution exponentielle. Pour commencer, on cherche alors à écrire une méthode $exp(rate)$ qui échantillonne des variables d'une loi exponentielle selon le tableau d'intensités $rate$ passé en paramètre. Cet échantillonnage se fera par **Inverse Transform Sampling**. Pour éviter les divisions par 0, on ajoutera $1e-200$ aux intensités qui valent 0.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-30T11:17:56.506237Z",
     "iopub.status.busy": "2021-11-30T11:17:56.505519Z",
     "iopub.status.idle": "2021-11-30T11:17:57.053941Z",
     "shell.execute_reply": "2021-11-30T11:17:57.053190Z",
     "shell.execute_reply.started": "2021-11-30T11:17:56.506109Z"
    },
    "id": "f1aE9ijomC1j"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "def exp(rate): # note: rate correspond a lambda dans les formules usuelles de la loi exponentielle\n",
    "  #>>>>>>>>>>\n",
    "  # votre code ici\n",
    "  # 1. Avoir calcule x = F(u, rate) comme en TD\n",
    "  # 2. Retourner un tirage aleatoire a partir de np.random.rand\n",
    "  # note: si on donne plusieurs valeur de rate, on fera autant de tirages (comme ci-dessous)\n",
    "  #<<<<<<<<<<\n",
    "    return np.random.exponential(1.0/np.where(rate !=0, rate, rate+1e-200))\n",
    "#Test : on sait que l'esperance de la loi exp est 1/lambda \n",
    "a=exp(np.array([[1,2,3],[4,5,6]]))\n",
    "for i in range(10000):\n",
    "    a+=exp(np.array([[1,2,3],[4,5,6]]))\n",
    "print(a/10000) # calcul de l'esperance\n",
    "\n",
    "# Pour comparaison avec la methode de reference de numpy:\n",
    "# ATTENTION, la methode attend un parametre 1/lambda (et non lambda)\n",
    "a=np.random.exponential(1.0/np.array([[1,2,3],[4,5,6]]))\n",
    "for i in range(10000):\n",
    "    a+=np.random.exponential(1.0/np.array([[1,2,3],[4,5,6]]))\n",
    "print(a/10000) # calcul de l'esperance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J2FBZBggg1B7"
   },
   "source": [
    "Verification :\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "[[0.98796784 0.49198855 0.33501196]\n",
    " [0.25022762 0.19644862 0.16723749]]\n",
    "[[1.00356177 0.50416273 0.34028414]\n",
    " [0.25231623 0.20024732 0.16911951]]\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QHJPFXBKoqIf"
   },
   "source": [
    "Soit le graphe de diffusion donne ci dessous: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-30T11:17:57.056802Z",
     "iopub.status.busy": "2021-11-30T11:17:57.056547Z",
     "iopub.status.idle": "2021-11-30T11:17:57.066095Z",
     "shell.execute_reply": "2021-11-30T11:17:57.065375Z",
     "shell.execute_reply.started": "2021-11-30T11:17:57.056770Z"
    },
    "id": "eBihGMdL7tZw"
   },
   "outputs": [],
   "source": [
    "names={0:\"Paul\",1:\"Jean\",2:\"Hector\",3:\"Rose\",4:\"Yasmine\",5:\"Leo\",6:\"Amine\",7:\"Mia\",8:\"Quentin\",9:\"Gaston\",10:\"Louise\"}\n",
    "k={(0,1):0.9,(1,0):0.9,(1,2):0.2,(2,3):0.5,(3,2):0.4,(2,4):0.9,(4,3):0.9,(1,3):0.5,(2,5):0.5,(5,7):0.7,(1,6):0.2,(6,7):0.1,(1,8):0.8,(8,9):0.2,(1,10):0.5,(10,9):0.9,(8,1):0.8}\n",
    "r={(0,1):0.2,(1,0):3,(1,2):1,(2,3):0.2,(3,2):0.5,(2,4):10,(4,3):2,(1,3):2,(2,5):0.5,(5,7):15,(1,6):3,(6,7):4,(1,8):0.8,(8,9):0.1,(1,10):12,(10,9):1,(8,1):14}\n",
    "graph=(names,k,r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cx2XlKT97sbh"
   },
   "source": [
    "La fonction display_graph ci dessous permet de visualiser le graphe de diffusion correspondant: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-30T11:17:57.068219Z",
     "iopub.status.busy": "2021-11-30T11:17:57.067723Z",
     "iopub.status.idle": "2021-11-30T11:17:58.169034Z",
     "shell.execute_reply": "2021-11-30T11:17:58.168302Z",
     "shell.execute_reply.started": "2021-11-30T11:17:57.068179Z"
    },
    "id": "M-etZqDj3PXW",
    "outputId": "c1ce6ed9-a5cd-4148-f6d6-885cf77a9ff9"
   },
   "outputs": [],
   "source": [
    "import pydot\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "style = { \"bgcolor\" : \"#6b85d1\", \"fgcolor\" : \"#FFFFFF\" }\n",
    "\n",
    "def display_graph ( graph_data, style, graph_name=\"diffusion_graph\" ):\n",
    "    graph = pydot.Dot( graph_name , graph_type='digraph')\n",
    "    names,k,r=graph_data\n",
    "    # creation des noeuds du reseau\n",
    "    for (i,name) in names.items():\n",
    "        new_node = pydot.Node( str(i)+\"_\"+name,\n",
    "                               style=\"filled\",\n",
    "                               fillcolor=style[\"bgcolor\"],\n",
    "                               fontcolor=style[\"fgcolor\"] )\n",
    "        graph.add_node( new_node )\n",
    "\n",
    "    # creation des arcs\n",
    "    for edge,valk in k.items():\n",
    "        valr=r[edge]\n",
    "        n1=str(edge[0])+\"_\"+names[edge[0]]\n",
    "        n2=str(edge[1])+\"_\"+names[edge[1]]\n",
    "        new_edge = pydot.Edge ( n1, n2, label=\"k=\"+str(valk)+\",r=\"+str(valr))\n",
    "        graph.add_edge ( new_edge )\n",
    "\n",
    "    # sauvegarde et affichage\n",
    "    outfile = graph_name + '.png'\n",
    "    graph.write_png( outfile )\n",
    "    img = mpimg.imread ( outfile )\n",
    "    plt.imshow( img )\n",
    "display_graph(graph,style)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s0_Ol6AgQa9g"
   },
   "source": [
    "On souhaite etre capable d'estimer les probabilites marginales d'infection des differents utilisateurs du reseau par une information pour laquelle on connait les sources (i.e., les utilisateurs infectes au temps 0). \n",
    "\n",
    "Etant donnes les cycles possibles dans le graphe de diffusion, considerer un calcul exact des probabilites d'infection des differents utilisateurs sachant le debut de la diffusion est inenvisageable : il faudrait considerer toutes les combinaisons possibles (infinies) de temps d'infection pour tous les utilisateurs non sources. \n",
    "\n",
    "Une possibilite pour calculer ces probabilites d'infections est de travailler par echantillonnage de Monte Carlo: on realise $n$ tirages d'infections connaissant les sources et on recense le ratio des simulations dans lesquelles chacun des utilisateurs est infecte avant un temps $maxT$.  \n",
    "\n",
    "L'idee est alors dans un premier temps d'ecrire une methode $simulation(graph,sources)$ qui, a partir d'une liste de sources, retourne les temps d'infection de l'ensemble des noeuds en fin de diffusion, sous la forme d'un tableau ou chaque case $i$ contient le temps d'infection du noeud $i$. Si le noeud $i$ n'a pas ete infecte ou bien si il l'a ete apres un temps maximal $maxT$, la case $i$ contient alors la valeur $maxT$. \n",
    "\n",
    "Le pseudo-code de la methode de simulation est donne ci dessous, avec $t_i$ le temps d'infection courant du noeud $i$:\n",
    "```\n",
    "ti=maxT pour tout i non source \n",
    "Tant qu'il reste des infectieux dont le temps est < maxT:\n",
    "  i=Infectieux de temps d'infection minimal\n",
    "  Pour tout noeud j tel que tj>ti:\n",
    "    sampler x selon Bernoulli(kij)\n",
    "    si x==1:\n",
    "       sampler delta selon Exp(rij)\n",
    "       t=ti+delta  \n",
    "       si t<tj: tj=t \n",
    "  Retrait de i de la liste des infectieux\n",
    "```\n",
    "Completez le code de la fonction donnee ci-dessous: \n",
    "\n",
    "**Note:** les resultats de reference ne seront obtenus que si on fait les appels a random dans le meme ordre que dans dans la correction de reference... Ce sera le cas si vous suivez les consignes detaillees ci-dessous. Mais vous pouvez aussi tenter de travailler directement a partir de l'algorithme ci-dessus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-30T11:17:58.172330Z",
     "iopub.status.busy": "2021-11-30T11:17:58.171229Z",
     "iopub.status.idle": "2021-11-30T11:17:58.194565Z",
     "shell.execute_reply": "2021-11-30T11:17:58.193804Z",
     "shell.execute_reply.started": "2021-11-30T11:17:58.172286Z"
    },
    "id": "FgfnYxbNTGDa"
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "maxT=10\n",
    "\n",
    "# returns dense numpy arrays of k,r parameters for graph links fr -> to \n",
    "def get_kr_for(graph,fr,to):\n",
    "    _,gk,gr=graph\n",
    "    k=np.array([[gk.get((i, v),0) for v in to] for i in fr])\n",
    "    r=np.array([[gr.get((i, v),0) for v in to] for i in fr])\n",
    "    return k,r\n",
    "\n",
    "def simulation(graph,sources, maxT):\n",
    "    #>>>>>>>>>>>>>>>>\n",
    "    # votre code ici:\n",
    "    nbNodes=len(names)\n",
    "    ti=np.zeros((len(graph[0])))\n",
    "    ti+=maxT #tous les ti à maxT\n",
    "    infectieux=np.zeros((len(graph[0])))  #si infectieux[i]=1: possibilite d'infecte\n",
    "    for i in sources:\n",
    "        ti[i] = 0  #pour les sources le ti=0\n",
    "        infectieux[i] = 1\n",
    "    while np.any(infectieux) == True:  #tant qu'il reste des infectieux\n",
    "        i = np.where(infectieux == 1)\n",
    "        infectieux_time = ti[i]  #temps des infectieux\n",
    "        i_min = i[0][np.argmin(infectieux_time)] #temps minimal\n",
    "        noeuds = []  #noeuds de l'infecte\n",
    "        for key in graph[1].keys():\n",
    "            if key[0] == i_min:\n",
    "                noeuds.append(key[1])\n",
    "        for j in noeuds:\n",
    "            if ti[j] > ti[i_min]:\n",
    "                k = graph[1].get((i_min,j))\n",
    "                x = np.random.binomial(1, k, 1)\n",
    "                if x == 1:\n",
    "                    r = graph[2].get((i_min,j))\n",
    "                    delta = exp(r)\n",
    "                    t = ti[i_min] + delta\n",
    "                    if t < ti[j]:\n",
    "                        ti[j] = t\n",
    "                        infectieux[j] = 1\n",
    "        infectieux[i_min] = 0\n",
    "    return ti\n",
    "        \n",
    "    # infectious sera le vecteur de travail dans lequel on elimine \n",
    "    # les noeuds traites\n",
    "    # => On cree aussi un vecteur times, qui sera celui contentant les\n",
    "    # temps de reference a retourner\n",
    "\n",
    "    #times = np.copy(infectious) \n",
    "    #while True: # boucle infinie (il faudra une clause en break)\n",
    "        # trouver le noeud contaminant a cette iteration = argmin dans infectious\n",
    "        # trouver le temps associe a la contamination: Tref\n",
    "        # eliminer le noeud en mettant sa valeur ) maxT dans infectious => il ne sera plus selectionne\n",
    "        # critere de sortie: il n'y a plus de noeuds contaminant possible \n",
    "        # trouver les indices des cibles (temps de contamination > Tref)\n",
    "        # trouver les parametres des modeles entre le noeud source et les cibles:\n",
    "       # params = get_kr_for(graph,[contaminant],cibles) # recuperation des parametres vers les cibles\n",
    "        # tirage Bernoulli selon params[0][0]: les cibles sont elles contaminees\n",
    "        # tirage Exp selon params[1][0]: quand est ce que les cibles sont contaminees (Tref + tirage)\n",
    "        # ce temps est-il infereur au temps auquel la cible aurait deja ete contaminee?\n",
    "        #    -> ne pas confondre np.min et np.minimum !\n",
    "        # mettre a jour times\n",
    "        # mettre a jour infectious\n",
    "    #<<<<<<<<<<<\n",
    "\n",
    "np.random.seed(1)\n",
    "print(simulation(graph,[0], maxT))\n",
    "print(simulation(graph,[0], maxT))\n",
    "print(simulation(graph,[0], maxT))\n",
    "np.random.seed(1)\n",
    "print(simulation(graph,[0,1], maxT))\n",
    "print(simulation(graph,[0,1], maxT))\n",
    "print(simulation(graph,[0,1], maxT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uxs-LQ3NgqHC"
   },
   "source": [
    "Verification : \n",
    "\n",
    "```\n",
    "[ 0.          2.71669685 10.         10.         10.         10.\n",
    " 10.         10.          3.19055869  3.17528764  2.86665883]\n",
    "[ 0.          0.60940319 10.         10.         10.         10.\n",
    " 10.         10.          2.36988928 10.         10.        ]\n",
    "[ 0.          0.22787406 10.         10.         10.         10.\n",
    " 10.         10.          1.27950225  3.42920125 10.        ]\n",
    "[ 0.          0.          0.03983788  0.09306264  0.05063365  1.10889995\n",
    " 10.          1.16647819 10.          1.16739272  0.03159079]\n",
    "[ 0.          0.         10.         10.         10.         10.\n",
    "  0.16359844 10.          1.71855838 10.         10.        ]\n",
    "[ 0.          0.          3.08047501  1.49963044  3.25699405 10.\n",
    " 10.         10.          0.83189232  2.23597755 10.        ]\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mPpzbeS_UMXk"
   },
   "source": [
    "La methode $getProbaMC(graph,sources,nbsimu)$ retourne les estimations de probabilites marginales d'infection des differents noeuds de $graph$, conditionnees a l'observation des  $sources$. Pour etre enregistree, une infection doit intervenir avant la seconde $maxT$. Ainsi, si la methode retourne 0.2 pour le noeud $i$, cela indique qu'il a ete infecte avec un temps $t_i \\in ]0,maxT[$ dans 20% des $nbsimu$ simulations effectuees. Completer la methode ci dessous: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-30T11:17:58.196720Z",
     "iopub.status.busy": "2021-11-30T11:17:58.196228Z",
     "iopub.status.idle": "2021-11-30T11:19:49.373662Z",
     "shell.execute_reply": "2021-11-30T11:19:49.372864Z",
     "shell.execute_reply.started": "2021-11-30T11:17:58.196685Z"
    },
    "id": "dV9zdHYPG7Op"
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "def getProbaMC(graph,sources, maxT, nbsimu=100000):\n",
    "    names,gk,gr=graph # eclatement du graphe\n",
    "    nbNodes=len(names)\n",
    "    rInf= np.zeros(nbNodes) \n",
    "    \n",
    "    # nb d'infection de chaque noeud dans la simulation suivante\n",
    "    #>>>>>>>>>>>\n",
    "    # votre code ici\n",
    "    # boucle for sur nbsimu\n",
    "    #   Realisation d'une simulation\n",
    "    #   Increment pour les noeuds contamines dans la simulation\n",
    "    # retour de rInf (normalise en frequence et pas en comptage)\n",
    "    #<<<<<<<<<<<\n",
    "    rInf=np.zeros((len(graph[0])))\n",
    "    for i in range(nbsimu):\n",
    "        simul=simulation(graph,sources,maxT)\n",
    "        simul=np.where(simul<maxT,simul,simul-10)  #on soustrait les non-infectes=maxT\n",
    "        simul=np.where(simul==0,simul,simul-simul+1) #on met des 1 la ou ya infection\n",
    "        rInf+=simul\n",
    "    rInf[sources]=nbsimu  #les sources sont toujours infecte\n",
    "    return rInf/nbsimu\n",
    "\n",
    "rInf=getProbaMC(graph,[0], maxT)\n",
    "print(rInf) \n",
    "\n",
    "rInf=getProbaMC(graph,[0], maxT)\n",
    "print(rInf)\n",
    "\n",
    "rInf=getProbaMC(graph,[0,1], maxT)\n",
    "print(rInf)  \n",
    "\n",
    "rInf=getProbaMC(graph,[2,8], maxT)\n",
    "print(rInf) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2VB69myqgYia"
   },
   "source": [
    "Verification : \n",
    "\n",
    "\n",
    "```\n",
    "[1.      0.7785  0.25939 0.44694 0.23214 0.11123 0.15518 0.09145 0.58973\n",
    " 0.36455 0.38976]\n",
    "[1.      0.77994 0.25928 0.44709 0.23307 0.11118 0.155   0.09067 0.59052\n",
    " 0.36201 0.38788]\n",
    "[1.      1.      0.35724 0.58993 0.32084 0.17582 0.20088 0.13995 0.79891\n",
    " 0.49967 0.49876]\n",
    "[0.71818 0.79804 1.      0.93559 0.89997 0.49813 0.15957 0.35803 1.\n",
    " 0.44108 0.39904]\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X-sluTACtRCM"
   },
   "source": [
    "Cette methode permet de bonnes estimations (malgre une certaine variance) lorsque l'on n'a pas d'observations autres que le vecteur de sources (i.e., on estime des probabilites de la forme: $P(t_i < maxT|\\{(j,t_j),t_j=0\\})$). Par contre, si l'on souhaite obtenir des probabilites d'infection du type $P(t_i < maxT|\\{(j,t_j),t_j=0\\}, \\{(j,t_j), j \\in {\\cal O}\\})$, c'est a dire conditionnees a des observations supplementaires pour un sous-ensembles de noeuds ${\\cal O}$ (avec $t_j > 0$ pour tout noeud $j$ de ${\\cal O}$), l'utilisation de la methode de MonteCarlo precedente est impossible. Cela impliquerait de filtrer les simulations obtenues selon qu'elles remplissent les conditions sur les noeuds de ${\\cal O}$, ce qui nous amenerait a toutes les ecarter sachant que l'on travaille avec des temps continus. \n",
    "\n",
    "Pour estimer ce genre de probabilite conditionnelle, nous allons nous appuyer sur des methodes de type MCMC, notamment la methode de Gibbs Sampling. Cette methode est utile pour simuler selon une loi jointe, lorsqu'il est plus simple d'echantillonner de chaque variable conditionnellement a toutes les autres plutot que directement de cette loi jointe. L'algorithme est donne par: \n",
    "\n",
    "\n",
    "1.   Tirage d'un vecteur de valeurs initiales pour toutes les variables $X_i$\n",
    "2.   Pour toutes les variable $X_i$ choisies dans un ordre aleatoire, echantillonnage d'une nouvelle valeur: $X_i \\sim p(x_i\\mid x_1,\\dots,x_{i-1},x_{i+1},\\dots,x_n)$\n",
    "3.   Recommencer en 2 tant qu'on souhaite encore des echantillons\n",
    "\n",
    "Notons qu'il est souvent utile d'exploiter la relation suivante, qui indique que pour echantillonner de la loi conditionnelle, il suffit d'echantillonner chaque variable proportionnellement a la loi jointe, avec toutes les autres variables fixees: \n",
    "$$p(x_j\\mid x_1,\\dots,x_{j-1},x_{j+1},\\dots,x_n) = \\frac{p(x_1,\\dots,x_n)}{p(x_1,\\dots,x_{j-1},x_{j+1},\\dots,x_n)} \\propto p(x_1,\\dots,x_n)$$\n",
    "\n",
    "Apres une periode dite de $burnin$ d'un nombre d'epoques a definir, l'algorithme emet des echantillons qui suivent la loi jointe connaissant les observations. Lorsque l'objectif est d'estimer des probabilites marginales, on fait alors tourner cet algorithme pendant une certain nombre d'epoques apres la periode de $burnin$, au cours desquelles on recence les differentes affectations de chacune des variables etudiees. \n",
    "\n",
    "Pour mettre en oeuvre cet algorithme, nous aurons aurons besoin d'avoir acces rapidement aux predecesseurs et successeurs dans le graphe. La methode ci-dessous retourne un couple de dictionnaires a partir du graphe: \n",
    " \n",
    "\n",
    "*   $preds[i]$  contient la liste des predecesseurs du  noeud $i$, sous la forme d'une liste de triplets $(j,k_{j,i},r_{j,i})$ pour tous les $j$ precedant $i$ dans le graphe.    \n",
    "*   $succs[i]$  contient la liste des successeurs du  noeud $i$, sous la forme d'une liste de triplets $(j,k_{i,j},r_{i,j})$ pour tous les $j$ pointes par $i$ dans le graphe.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-30T11:19:49.375424Z",
     "iopub.status.busy": "2021-11-30T11:19:49.374815Z",
     "iopub.status.idle": "2021-11-30T11:19:49.384844Z",
     "shell.execute_reply": "2021-11-30T11:19:49.383868Z",
     "shell.execute_reply.started": "2021-11-30T11:19:49.375377Z"
    },
    "id": "0sPmzchnSP3r",
    "outputId": "66b3d635-6c9b-4bea-a6d7-4c9e2b60ff75"
   },
   "outputs": [],
   "source": [
    "# pre-calcul des prececesseurs et successeurs pour gagner du temps ensuite\n",
    "def getPredsSuccs(graph):\n",
    "    names,gk,gr=graph\n",
    "    nbNodes=len(names)\n",
    "    preds={}\n",
    "    succs={}\n",
    "    for (a,b),v in gk.items():\n",
    "        s=succs.get(a,[])\n",
    "        s.append((b,v,gr[(a,b)]))\n",
    "        succs[a]=s\n",
    "        p=preds.get(b,[])\n",
    "        p.append((a,v,gr[(a,b)]))\n",
    "        preds[b]=p\n",
    "    return (preds,succs)\n",
    "preds,succs=getPredsSuccs(graph)\n",
    "print(\"preds=\",preds)\n",
    "print(\"succs=\",succs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vk-vCmVX6HtV"
   },
   "source": [
    "Pour calculer les probabilites conditionnelles, il faut prendre en compte les quantites suivantes: \n",
    "\n",
    "\n",
    "*   Probabilite pour $j$ d'etre infecte par $i$ au temps $t_j$ connaissant $t_i < t_j$:  \n",
    "$$\\alpha_{i,j}=k_{i,j}r_{i,j} exp(-r_{i,j}(t_j-t_i))$$\n",
    "*   Probabilite pour $j$ de ne pas etre infecte par $i$ jusqu'au temps $t$:\n",
    "$$\\beta_{i,j}=k_{i,j} exp(-r_{i,j}(t_j-t_i)) + 1 - k_{i,j}$$\n",
    "*   Probabilite pour $j$ d'etre infecte au temps $t_j$ connaissant les predecesseurs infectes avant $t_j$:\n",
    "$$h_{j}=\\prod_{i \\in preds[j], t_i<t_j} \\beta_{i,j} \\sum_{i \\in preds[i], t_i<t_j} \\alpha_{i,j} / \\beta_{i,j}$$\n",
    "*   Probabilite pour $j$ de ne pas etre infecte avant $maxT$ connsaissant ses predecesseurs infectes:\n",
    "$$g_{j}=\\prod_{i \\in preds[j], t_i<t_j} \\left(k_{i,j} exp(-r_{i,j}(maxT-t_i)) + 1 - k_{i,j}\\right)=\\prod_{i \\in preds[j], t_i<t_j} \\beta_{i,j}$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Dans la methode $computeab(v, times, preds)$, on prepare le calcul et les mises a jour de ces quantites. La methode calcule, pour un noeud $v$ selon les temps d'infection courants donnes dans $times$, deux quantites $a$ et $b$: \n",
    "\n",
    "$$a= \\left\\{\n",
    "\\begin{array}{l}\n",
    "\\max(1e^{-20}, \\sum_{i \\in preds[v], t_i<t_v} \\alpha_{i,v} / \\beta_{i,v}) \\mbox{ si: } t_v< maxT \\\\\n",
    "1 \\mbox{ sinon }\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$ \n",
    "\n",
    "$$b=\\sum_{i \\in preds[v], t_i<t_v} \\log \\beta_{i,v}$$\n",
    "\n",
    "Si $v$ appartient aux sources, on retourne $(a,b)=(1,0)$\n",
    "\n",
    "Completer la methode $computeab$ donnee ci-dessous:   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-30T11:19:49.386791Z",
     "iopub.status.busy": "2021-11-30T11:19:49.386534Z",
     "iopub.status.idle": "2021-11-30T11:19:49.401894Z",
     "shell.execute_reply": "2021-11-30T11:19:49.400926Z",
     "shell.execute_reply.started": "2021-11-30T11:19:49.386756Z"
    },
    "id": "cOnGCJCBSulp"
   },
   "outputs": [],
   "source": [
    "eps=1e-20\n",
    "\n",
    "def computeab(v, times, preds, maxT, eps=1e-20):\n",
    "    preds=preds.get(v,[])\n",
    "    t=times[v]\n",
    "    if t==0:\n",
    "        return (1,0)\n",
    "    a=eps\n",
    "    b=0\n",
    "    if len(preds)>0:\n",
    "        c,k,r=map(np.array,zip(*preds))\n",
    "        somme_alpha_sur_beta=0\n",
    "        somme_log_beta=0\n",
    "        for j in range(len(preds)):\n",
    "            previous=preds[j][0]\n",
    "            k=preds[j][1]\n",
    "            r=preds[j][2]\n",
    "            tj=times[previous]\n",
    "            if tj<t:\n",
    "                alpha=k*r*np.exp(-r*(t-tj))\n",
    "                beta=k*np.exp(-r*(t-tj))+1-k\n",
    "                somme_alpha_sur_beta+=alpha/beta\n",
    "                somme_log_beta+=np.log(beta)\n",
    "        if t<maxT:\n",
    "            a=np.maximum(eps,somme_alpha_sur_beta)\n",
    "        else:\n",
    "            a=1.0\n",
    "        b=somme_log_beta\n",
    "    return (a,b)\n",
    "\n",
    "nbNodes=len(graph[0])\n",
    "times=np.array([maxT]*nbNodes,dtype=float)\n",
    "times[0]=0\n",
    "times[1]=1\n",
    "times[2]=4\n",
    "\n",
    "print(computeab(0,times,preds, maxT, eps))\n",
    "print(computeab(1,times,preds, maxT, eps))\n",
    "print(computeab(2,times,preds, maxT, eps))\n",
    "print(computeab(3,times,preds, maxT, eps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ujNcsUdcfKYR"
   },
   "source": [
    "Verification : \n",
    "\n",
    "\n",
    "```\n",
    "(1, 0)\n",
    "(0.17610107365772135, -0.17810126145719926)\n",
    "(0.012293749653343877, -0.2107736084094422)\n",
    "(1.0, -1.12301187855188)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aXnKXCMfLMK-"
   },
   "source": [
    "La methode $computell$ calcule la log-vraisemblance d'une diffusion (representee par le tableau times), en appelant la methode computeab sur l'ensemble des noeuds du reseau. Elle retourne un triplet (log-likelihood, sa, sb), avec $sa$ et $sb$ les tables des valeurs $a$ et $b$ pour tous les noeuds.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-30T11:19:49.403755Z",
     "iopub.status.busy": "2021-11-30T11:19:49.403302Z",
     "iopub.status.idle": "2021-11-30T11:19:49.416344Z",
     "shell.execute_reply": "2021-11-30T11:19:49.415417Z",
     "shell.execute_reply.started": "2021-11-30T11:19:49.403716Z"
    },
    "id": "aM0K-VhPUXJn"
   },
   "outputs": [],
   "source": [
    "def computell(times,preds, maxT, eps):\n",
    "    ll=0.0\n",
    "    sa=np.zeros((len(times)))\n",
    "    sb=np.zeros((len(times)))\n",
    "    for i in range(len(times)):\n",
    "        a,b=computeab(i,times,preds,maxT)\n",
    "        sa[i]=a\n",
    "        sb[i]=b\n",
    "        ll+=np.log(a)+b\n",
    "    return ll,sa,sb\n",
    "\n",
    "ll,sa,sb=computell(times,preds, maxT, eps)\n",
    "print(\"ll=\",ll)\n",
    "print(times)\n",
    "print(\"like_indiv=\",np.exp(np.log(sa)+sb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "akb4kVy3hK2X"
   },
   "source": [
    "Verification : \n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "ll= -13.117139892397578\n",
    "[ 0.  1.  4. 10. 10. 10. 10. 10. 10. 10. 10.]\n",
    "like_indiv= [1.         0.14737154 0.00995741 0.32529856 0.1        0.52489353\n",
    " 0.8        1.         0.20059727 1.         0.5       ]\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aNlDzJeFNb60"
   },
   "source": [
    "Afin de preparer les mises a jour lors des affectations successives des variables du Gibbs Sampling, on propose de definir une methode $removeV(v,times,succs,sa,sb)$ qui retire temporairement du reseau un noeud $v$, en passant son temps d'infection a -1 dans times et en retirant sa contribution aux valeurs a et b (contenues dans sa et sb) de tous ses successeurs $j$ tels que $t_j > t_v$ (y compris donc les non infectes qui sont a $t_j=maxT$). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-30T11:19:49.418025Z",
     "iopub.status.busy": "2021-11-30T11:19:49.417576Z",
     "iopub.status.idle": "2021-11-30T11:19:49.439288Z",
     "shell.execute_reply": "2021-11-30T11:19:49.438443Z",
     "shell.execute_reply.started": "2021-11-30T11:19:49.417985Z"
    },
    "id": "rEGzpS_DaRX5",
    "outputId": "f1cd5174-87e0-426e-f665-f4a1b2ed89f5"
   },
   "outputs": [],
   "source": [
    "def removeV(v,times,succs,sa,sb):\n",
    "    succs=succs.get(v,[])\n",
    "    t=times[v]\n",
    "    if t<0:\n",
    "        return \n",
    "    times[v]=-1\n",
    "    sa[v]=1.0\n",
    "    sb[v]=0.0\n",
    "    if len(succs)>0:\n",
    "        c,k,r=map(np.array,zip(*succs))\n",
    "        tp=times[c]\n",
    "        which=(tp>t)\n",
    "\n",
    "        tp=tp[which]\n",
    "        dt=tp-t\n",
    "        k=k[which]\n",
    "        r=r[which]\n",
    "        c=c[which]\n",
    "        rt = -r*dt\n",
    "        b1=k*np.exp(rt)\n",
    "        b=b1+1.0-k\n",
    "\n",
    "        a=r*b1\n",
    "        a=a/b\n",
    "        b=np.log(b)\n",
    "\n",
    "        sa[c]=sa[c]-np.where(tp<maxT,a,0.0)\n",
    "        sa[c]=np.where(sa[c]>eps,sa[c],eps)\n",
    "        sb[c]=sb[c]-b\n",
    "        sb[c]=np.where(sb[c]>0,0,sb[c])\n",
    "\n",
    "#Test\n",
    "print(\"sa=\",sa)\n",
    "print(\"sb=\",sb)\n",
    "\n",
    "nsa=np.copy(sa)\n",
    "nsb=np.copy(sb)\n",
    "ntimes=np.copy(times)\n",
    "removeV(3,ntimes,succs,nsa,nsb)\n",
    "print(\"diffa=\",nsa-sa)\n",
    "print(\"diffb=\",nsb-sb)\n",
    "\n",
    "nsa=np.copy(sa)\n",
    "nsb=np.copy(sb)\n",
    "ntimes=np.copy(times)\n",
    "removeV(1,ntimes,succs,nsa,nsb)\n",
    "print(\"diffa=\",nsa-sa)\n",
    "print(\"diffb=\",nsb-sb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cb8iIMwgO6D9"
   },
   "source": [
    "La methode addVatT fait l'inverse: elle rajoute un noeud qui etait retire du reseau, avec un temps $newt$. Il faut alors mettre a jour les valeurs a et b (dans sa et sb) de tous les successeurs de $v$ tels que $t_j > newt$ et calculer les valeurs a et b du noeud v. \n",
    "\n",
    "Completer le code ci-dessous: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-30T11:19:49.443219Z",
     "iopub.status.busy": "2021-11-30T11:19:49.442551Z",
     "iopub.status.idle": "2021-11-30T11:19:49.468028Z",
     "shell.execute_reply": "2021-11-30T11:19:49.466644Z",
     "shell.execute_reply.started": "2021-11-30T11:19:49.443150Z"
    },
    "id": "9sDiLiXnngog"
   },
   "outputs": [],
   "source": [
    "def addVatT(v,times,newt,preds,succs,sa,sb):\n",
    "    t=times[v]\n",
    "    if t>=0:\n",
    "        raise Error(\"v  must have been removed before\")\n",
    "  #>>>>>>>>>>>>\n",
    "  # votre code ici\n",
    "  #<<<<<<<<<<<<<<<<<<<\n",
    "    times[v]=newt\n",
    "    sa[v],sb[v]=computeab(v,times,preds,maxT) #a et b pour le noeud supprime\n",
    "    for i in preds:  \n",
    "        sa[i],sb[i]=computeab(i,times,preds,maxT) #mise a jour des valeurs\n",
    "\n",
    "# Tests: \n",
    "nsa=np.copy(sa)\n",
    "nsb=np.copy(sb)\n",
    "c,_,_=map(np.array,zip(*succs[1]))\n",
    "c=np.append(c,1)\n",
    "ll=np.sum((np.log(nsa)+nsb)[c])           # somme des logvraisemblances pouvant être modifiées par la modification du temps de 1 (avant modif)\n",
    "removeV(1,times,succs,nsa,nsb)\n",
    "addVatT(1,times,2,preds,succs,nsa,nsb)\n",
    "ll2=np.sum((np.log(nsa)+nsb)[c])          # somme des logvraisemblances pouvant avoir été modifiées par la modification du temps de 1 (après modif)\n",
    "removeV(1,times,succs,nsa,nsb)\n",
    "addVatT(1,times,1,preds,succs,nsa,nsb)\n",
    "ll3=np.sum((np.log(nsa)+nsb)[c])          # somme des logvraisemblances pouvant  avoir été modifiées par la modification du temps de 1 (après remise dans l'état initial)\n",
    "llall=np.sum(np.log(nsa)+nsb)             # logvraisemblance globale\n",
    "print(np.exp(ll),np.exp(ll2),np.exp(ll3),llall)\n",
    "\n",
    "c,_,_=map(np.array,zip(*succs[0]))\n",
    "c=np.append(c,0)\n",
    "ll=np.sum((np.log(nsa)+nsb)[c])\n",
    "removeV(0,times,succs,nsa,nsb)\n",
    "addVatT(0,times,maxT,preds,succs,nsa,nsb)\n",
    "ll2=np.sum((np.log(nsa)+nsb)[c])\n",
    "removeV(0,times,succs,nsa,nsb)\n",
    "addVatT(0,times,0,preds,succs,nsa,nsb)\n",
    "ll3=np.sum((np.log(nsa)+nsb)[c])\n",
    "llall=np.sum(np.log(nsa)+nsb)\n",
    "print(np.exp(ll),np.exp(ll2),np.exp(ll3),llall)\n",
    "\n",
    "c,_,_=map(np.array,zip(*succs[5]))\n",
    "c=np.append(c,5)\n",
    "ll=np.sum((np.log(nsa)+nsb)[c])\n",
    "removeV(5,times,succs,nsa,nsb)\n",
    "addVatT(5,times,1,preds,succs,nsa,nsb)\n",
    "ll2=np.sum((np.log(nsa)+nsb)[c])\n",
    "removeV(5,times,succs,nsa,nsb)\n",
    "addVatT(5,times,maxT,preds,succs,nsa,nsb)\n",
    "ll3=np.sum((np.log(nsa)+nsb)[c])\n",
    "llall=np.sum(np.log(nsa)+nsb)\n",
    "print(np.exp(ll),np.exp(ll2),np.exp(ll3),llall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pBg9T8k2hhmO"
   },
   "source": [
    "Verification : \n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "3.830251606174211e-05 8.555487921315824e-05 3.830251606174211e-05 -13.117139892397578\n",
    "0.14737153555403676 1.0000000000169125e-21 0.14737153555403676 -13.117139892397578\n",
    "0.5248935341839319 2.999999999999998e-21 0.5248935341839319 -13.117139892397578\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3isBRUNi3JPA"
   },
   "source": [
    "Pour echantillonner pour une variable $i$, il faudra etre a meme de comparer les vraisemblances selon les differentes affectations. Cela implique de calculer la somme de toutes ces vraisemblances. Mais pour realiser cette somme, il faudrait que nous sortions de la representation logarithmique: $\\sum_{t_i} exp(log(p(t_1,\\dots,t_i,\\dots,t_n))$. Si on le fait de cette maniere, on risque d'avoir des arrondis a 0 presque partout. Une possibilite (log-sum-exp trick) est d'exploiter la relation suivante:  \n",
    "\n",
    "$$\\log\\sum_i x_i = x^* + \\log\\left( \\exp(x_1-x^*)+ \\cdots + \\exp(x_n-x^*) \\right)$$\n",
    "avec $x^* = \\max{\\{x_1, \\dots, x_n\\}}$\n",
    "\n",
    "Completer la methode logsumexp suivante, qui realise cette somme en evitant les problemes numeriques: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-30T11:19:49.470685Z",
     "iopub.status.busy": "2021-11-30T11:19:49.470326Z",
     "iopub.status.idle": "2021-11-30T11:19:49.478276Z",
     "shell.execute_reply": "2021-11-30T11:19:49.477488Z",
     "shell.execute_reply.started": "2021-11-30T11:19:49.470651Z"
    },
    "id": "SZuOJI7B3p0O",
    "outputId": "dd48b42f-6b96-4121-9a1c-291e8468288f"
   },
   "outputs": [],
   "source": [
    "def logsumexp(x,axis=-1):\n",
    "  #>>>>>>>>>>\n",
    "  # votre code ici\n",
    "  #<<<<<<<<<<\n",
    "    xe=np.amax(x)\n",
    "    return xe+np.log(np.sum(np.exp(x-xe),axis))\n",
    " \n",
    "# Test: \n",
    "x=np.array([[0.001,0.02,0.008],[0.1,0.01,0.4]])\n",
    "r=np.log(np.sum(x,-1))\n",
    "x=np.log(x)\n",
    "r2=logsumexp(x)\n",
    "print(r2,r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ZpzgRBZhtOm"
   },
   "source": [
    "Verification : \n",
    "\n",
    "\n",
    "```\n",
    "[-3.54045945 -0.67334455] [-3.54045945 -0.67334455]\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oqVI0e9T85x-"
   },
   "source": [
    "On souhaite maintenant mettre en place une methode $sampleV(v,times,newt,preds,succs,sa,sb,k,k2)$ qui sample un nouveau temps d'infection pour le noeud $v$, connaissant les temps de tous les autres noeuds dans $times$ (ainsi que leurs valeurs $a$ et $b$ correspondantes contenues dans sa et sb). Puisque le domaine de support de $t_v$ est continu, on doit faire quelques approximations en se basant sur une discretisation des valeurs possibles:\n",
    "\n",
    "1.   On decoupe la plage de temps $[0;maxT]$ en $k$ bins reguliers. Dans chaque bin $i$, on echantillonne uniformement un temps, pour obtenir $k$ points $d_1,\\dots,d_k$. Si $t_v < maxT$, on ajoute $t_v$ a cet ensemble de points pour gagner en stabilite (insere dans la liste de maniere a conserver l'ordre croissant). \n",
    "2.   On considere chaque point $d_i$ comme le prototype d'un bin $[(d_i+d_{i-1})/2,(d_i+d_{i+1})/2]$. Pour $d_1$ on prend $[0,(d_1+d_2)/2]$ et pour $d_k$ on prend   $[(d_k+d_{k-1})/2,maxT]$. On fait l'hypothese que la densite de probabilite est constante sur l'ensemble de chaque bin $i$, que l'on evalue en  $t_v=d_i$.   La probabilite que l'on echantillonne dans le bin $i$ est alors egale a: $p(t_v \\in bin_i | \\{t_u\\}_{ u \\in V\\setminus v}) =  \\frac{z_i \\times l_i}{\\sum_j z_j \\times l_j + z_{maxT}}$, avec $z_i$ la vraisemblance  calculee selon $t_v =d_i$, $l_i$ la taille du bin $i$ et $z_{maxT}$ la vraisemblance  calculee pour $t_v=maxT$. La probabilite que $v$ ne soit pas infecte dans la diffusion est alors donnee par : $p(t_v = maxT | \\{t_u\\}_{ u \\in V\\setminus v}) =  \\frac{z_{maxT}}{\\sum_j z_j \\times l_j + z_{maxT}}$.\n",
    "3. On echantillonne une variable $x$ proportionnellement aux probabilites calculees a l'etape precedente.  Si $x$ ne correspond pas a $maxT$, $v$ est alors infecte a un temps inclus dans l'intervale du bin correspondant a $x$. Il s'agit alors de re-echantillonner $k2$ points uniformement dans ce bin et de calculer les densites en ces points (pour gagner en stabilite on ajoute le prototype du bin $d_i$). Le nouveau temps de $v$ est alors echantillonne proportionnellement a ces densites.\n",
    "\n",
    "Le code de la methode de sampling est donne ci-dessous:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-30T11:19:49.480542Z",
     "iopub.status.busy": "2021-11-30T11:19:49.479965Z",
     "iopub.status.idle": "2021-11-30T11:19:49.545948Z",
     "shell.execute_reply": "2021-11-30T11:19:49.545210Z",
     "shell.execute_reply.started": "2021-11-30T11:19:49.480504Z"
    },
    "id": "K3QmBCRbhkLM"
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "def getLL(v,times,nt,preds,succs,sa,sb,onUsers=None):\n",
    "    sa=np.copy(sa)\n",
    "    sb=np.copy(sb)\n",
    "    if onUsers is None:\n",
    "        onUsers=range(len(times))\n",
    "    addVatT(v,times,nt,preds,succs,sa,sb)\n",
    "    times[v]=-1\n",
    "    ll=np.sum((np.log(sa)+sb)[onUsers])\n",
    "    return (ll,sa,sb)\n",
    "  \n",
    "def sampleV(v,times,preds,succs,sa,sb,k,k2):\n",
    "  \n",
    "    nbCandidateT=k\n",
    "    bounds=np.linspace(0,maxT,nbCandidateT)\n",
    "    newt=np.random.uniform(bounds[:-1],bounds[1:])\n",
    "\n",
    "    if times[v]<maxT:\n",
    "        idx = newt.searchsorted(times[v])\n",
    "        newt=np.concatenate((newt[:idx], [times[v]], newt[idx:]),axis=0)\n",
    "        nbCandidateT+=1\n",
    "    newt=np.append(newt,[maxT])\n",
    "\n",
    "    if v in succs:\n",
    "        c,_,_=map(list,zip(*succs.get(v,[])))\n",
    "    else:\n",
    "        c=[]\n",
    "    c.append(v)\n",
    "    c=np.array(c)\n",
    "    oldll=np.sum((np.log(sa)+sb)[c])\n",
    "    otime=times[v]\n",
    "    nsa=np.copy(sa)\n",
    "    nsb=np.copy(sb)\n",
    "    removeV(v,times,succs,nsa,nsb)\n",
    "    lls=[getLL(v,times,nt,preds,succs,nsa,nsb,onUsers=c) for nt in newt]\n",
    "    ll,la,lb=zip(*lls)\n",
    "    ll=list(ll)\n",
    "    ll=np.array(ll)\n",
    "\n",
    "    diffsx=(newt[1:]-newt[:-1])/2.0\n",
    "    diffsx[1:]=diffsx[1:]+diffsx[:-1]\n",
    "    diffsx[0]+=newt[0]\n",
    "    diffsx[-1]+=(maxT-newt[nbCandidateT-1])/2.0\n",
    "    areas=np.log(diffsx)+ll[:-1]\n",
    "    lln=np.append(areas,ll[-1])\n",
    "\n",
    "    p=np.exp(lln-logsumexp(lln))\n",
    "\n",
    "    i=np.random.choice(range(len(p)),1,p=p).sum()\n",
    "    if i==(len(p)-1):\n",
    "        times[v]=maxT\n",
    "        np.copyto(sa,np.array(la[-1]))\n",
    "        np.copyto(sb,np.array(lb[-1]))\n",
    "    else: \n",
    "        if i>0: \n",
    "            bi=(newt[i]+newt[i-1])/2.0\n",
    "        else:\n",
    "            bi=0\n",
    "        if i<(len(p)-2): \n",
    "            bs=(newt[i]+newt[i+1])/2.0\n",
    "        else:\n",
    "            bs=maxT\n",
    "        bounds=np.linspace(bi,bs,k2)\n",
    "        newt=np.concatenate(([newt[i]],np.random.uniform(bounds[:-1],bounds[1:])))\n",
    "        lls=[getLL(v,times,nt,preds,succs,nsa,nsb,onUsers=c) for nt in newt]\n",
    "        ll,la,lb=zip(*lls)\n",
    "        ll=np.array(ll)\n",
    "        p=np.exp(ll-logsumexp(ll))\n",
    "\n",
    "        i=np.random.choice(range(len(p)),1,p=p).sum()\n",
    "\n",
    "        times[v]=newt[i]\n",
    "        np.copyto(sa,np.array(la[i]))\n",
    "        np.copyto(sb,np.array(lb[i]))\n",
    "   \n",
    "times=np.array([maxT]*nbNodes,dtype=float)\n",
    "times[0]=0\n",
    "times[1]=1\n",
    "times[2]=4\n",
    "np.random.seed(0)\n",
    "print(times)\n",
    "sampleV(5,times,preds,succs,sa,sb,10,10)\n",
    "print(times)\n",
    "sampleV(5,times,preds,succs,sa,sb,10,10)\n",
    "print(times)\n",
    "sampleV(5,times,preds,succs,sa,sb,10,10)\n",
    "print(times)\n",
    "sampleV(5,times,preds,succs,sa,sb,10,10)\n",
    "print(times)\n",
    "sampleV(5,times,preds,succs,sa,sb,10,10)\n",
    "print(times)\n",
    "sampleV(5,times,preds,succs,sa,sb,10,10)\n",
    "print(times)\n",
    "sampleV(5,times,preds,succs,sa,sb,10,10)\n",
    "print(times)\n",
    "sampleV(5,times,preds,succs,sa,sb,10,10)\n",
    "print(times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0RMM_c2siHez"
   },
   "source": [
    "Verification : \n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "[ 0.  1.  4. 10. 10. 10. 10. 10. 10. 10. 10.]\n",
    "[ 0.  1.  4. 10. 10. 10. 10. 10. 10. 10. 10.]\n",
    "[ 0.  1.  4. 10. 10. 10. 10. 10. 10. 10. 10.]\n",
    "[ 0.  1.  4. 10. 10. 10. 10. 10. 10. 10. 10.]\n",
    "[ 0.  1.  4. 10. 10. 10. 10. 10. 10. 10. 10.]\n",
    "[ 0.  1.  4. 10. 10. 10. 10. 10. 10. 10. 10.]\n",
    "[ 0.  1.  4. 10. 10. 10. 10. 10. 10. 10. 10.]\n",
    "[ 0.          1.          4.         10.         10.          4.20931617\n",
    " 10.         10.         10.         10.         10.        ]\n",
    "[ 0.  1.  4. 10. 10. 10. 10. 10. 10. 10. 10.]\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UCdSA3NF8CrE"
   },
   "source": [
    "Completer la methode de Gibbs Sampling $gb$ ci-dessous, avec $k$ le nombre de bins a utiliser et $k2$ le nombre de points a echantillonner dans le bin choisi. Le parametre $ref$ correspond a un vecteur de probabilites marginales de reference (par exemple obtenu par MonteCarlo lorsque c'est possible) avec lequel on peut afficher la distance MSE au fur et a mesure du processus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xsnQHVcdjfVT",
    "outputId": "8c274aa7-e425-4ed7-d3a6-a114a85988b8"
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "def gb(graph,infections,burnin=1000,nbEpochs=10000,k=100,k2=50,ref=None):\n",
    "    #>>>>>>>>>>>>>>>>>>>>>>>\n",
    "    # votre code ici\n",
    "    #>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "    maxT=10\n",
    "    eps=1e-5\n",
    "    nbNodes=len(graph[0])\n",
    "    times_copy=times.copy()\n",
    "    preds,succs=getPredsSuccs(graph)\n",
    "    mses = []\n",
    "    times_arr = []\n",
    "    \n",
    "    for _ in range(nbEpochs):\n",
    "        for _ in range(nbNodes):\n",
    "            l,sa,sb=computell(times_copy,preds, maxT, eps)\n",
    "            v = np.random.choice(nbNodes, replace=False)\n",
    "            if i < burnin:\n",
    "                sampleV(v,times_copy,preds,succs,sa,sb,k,k2)\n",
    "            else: \n",
    "                sampleV(v,times_copy,preds,succs,sa,sb,k,k2)\n",
    "                times_arr.append(times_copy)\n",
    "                mses.append((times_copy-ref)**2)\n",
    "    return mses, times_arr, times_copy\n",
    "\n",
    "# On teste ici avec seulement des sources (i.e., des infectes au temps 0), car cela permet de comparer a la ref MonteCarlo (mais il faudrait aussi tester avec d'autres infectes : c'est l'objectif). \n",
    "ref=getProbaMC(graph,[0],maxT) \n",
    "mses,times_arr,last_times_copy=gb(graph,[(0,0)],burnin=100,nbEpochs=1000,ref=ref)\n",
    "# print(rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ref)\n",
    "print(times_arr.mean())\n",
    "print(last_times_copy)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(mses[::5000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iqx6CQ4vQ8rr"
   },
   "source": [
    "# Partie optionnelle\n",
    "\n",
    "L'algorithme de Metropolis-Hasting est une autre methode de type MCMC qui utilise une distribution d'echantillonnage pour se deplacer dans l'espace des points consideres. Il s'agit de definir une distribution $q(y_{t+1}|x_t)$ de laquelle on sait generer un deplacement. L'algorithme procede alors de la maniere suivante: \n",
    "\n",
    "\n",
    "1.   Generer $y_{t+1}$ selon $q(y_{t+1}|x_t)$ \n",
    "2.   Calculer la probabilite dâ€™acceptation $\\alpha(x_t,y_{t+1})=\\min\\left\\{\\frac{\\pi(y_{t+1})q(x_t|y_{t+1})}{\\pi(x_t)q(y_{t+1}|x_t)},1\\right\\} \\,\\!, \\text{ avec } \\pi(x_t) \\text{ la densite de probabilite de } x_t$\n",
    "3.   Prendre $x_{t+1}=\\begin{cases} y_{t+1}, & \\text{avec probabilite}\\,\\,\\alpha \\\\ x_t, & \\text{avec probabilite}\\,\\,1-\\alpha \\end{cases}$\n",
    "\n",
    "\n",
    "\n",
    "Dans notre cas, on propose de travailler avec des deplacements correspondants a des permutations d'un temps d'infection a chaque iteration, comme dans le cadre du Gibbs Sampling. A chaque etape on choisit donc une variable a modifier, on choisit un nouveau temps pour cette variable et on calcule la densite correspondante. La probabilite d'acceptation est ensuite calculee selon cette densite et la probabilite du deplacement selon la distribution $q$ qui a servi a generer le nouveau temps d'infection. On se propose de choisir $maxT$ avec une probabilite de 0.1. La probabilite $q(t_v|t)$ pour $t< maxT$ est alors egale  a  $0.9\\times \\frac{1}{maxT}$.\n",
    "\n",
    "Implementer l'approche d'echantillonnage par Metropolis-Hasting pour notre probleme d'estimation de probabilites marginales d'infection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-30T11:33:26.612840Z",
     "iopub.status.busy": "2021-11-30T11:33:26.612257Z",
     "iopub.status.idle": "2021-11-30T11:33:51.108827Z",
     "shell.execute_reply": "2021-11-30T11:33:51.108116Z",
     "shell.execute_reply.started": "2021-11-30T11:33:26.612802Z"
    },
    "id": "TGoHITZmGxg-",
    "outputId": "e463281e-3ce0-4bba-c8b3-c2b44fbf8f7c"
   },
   "outputs": [],
   "source": [
    "def mh(graph, infections, burnin=1000, ref=None):\n",
    "    return 0\n",
    "\n",
    "maxT=10\n",
    "ref=getProbaMC(graph,[0],maxT=10) \n",
    "rate=mh(graph,[(0,0)],burnin=100,ref=ref)\n",
    "print(rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
