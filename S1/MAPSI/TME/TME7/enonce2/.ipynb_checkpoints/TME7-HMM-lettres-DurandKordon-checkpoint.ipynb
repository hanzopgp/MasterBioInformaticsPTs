{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TME HMM : décodage des lettres\n",
    "\n",
    "Le but est le même que dans le TME précédent: apprendre à classer les lettres les lettres manuscrites enregistrées par un stylo intelligent.\n",
    "\n",
    "Les étapes sont un peu les mêmes que dans le TME précédent... Avec un modèle plus complexe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-f44a02b52a1c>:7: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  X = np.array(data.get('letters')) # récupération des données sur les lettres\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with open('ressources/lettres.pkl', 'rb') as f:\n",
    "    data = pkl.load(f, encoding='latin1') \n",
    "X = np.array(data.get('letters')) # récupération des données sur les lettres\n",
    "Y = np.array(data.get('labels')) # récupération des étiquettes associées \n",
    "\n",
    "nCl = 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# affichage d'une lettre (cf TME précédent)\n",
    "def tracerLettre(let):\n",
    "    a = -let*np.pi/180; # conversion en rad\n",
    "    coord = np.array([[0, 0]]); # point initial\n",
    "    for i in range(len(a)):\n",
    "        x = np.array([[1, 0]]);\n",
    "        rot = np.array([[np.cos(a[i]), -np.sin(a[i])],[ np.sin(a[i]),np.cos(a[i])]])\n",
    "        xr = x.dot(rot) # application de la rotation\n",
    "        coord = np.vstack((coord,xr+coord[-1,:]))\n",
    "    plt.figure()\n",
    "    plt.plot(coord[:,0],coord[:,1])\n",
    "    #plt.savefig(\"exlettre.png\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discrétisation (cf TME précédent)\n",
    "def discretise(x, d):\n",
    "    intervalle = 360 / d\n",
    "    xd = [np.floor(x[i] / intervalle) for i in range(len(x))]\n",
    "    return np.array(xd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Apprentissage d'un modèle connaissant les états cachés du système\n",
    "\n",
    "### A1. Hypothèse gauche-droite\n",
    "\n",
    "On fait l'hypothèse que les états sont connus... Alors que ce n'est pas le cas. Mais il existe des stratégies simples (et parfois efficaces) pour attribuer arbitrairement des états sur les chaines.\n",
    "La plus classique est l'hypothèse gauche-droite, qui est bien adaptée aux signaux courts et non périodiques:\n",
    "\n",
    "* On définit le nombre d'états N\n",
    "* On découpe chaque série d'observations en N portions à peu près égales\n",
    "* On affecte l'état 0 au début, puis on incrémente jusqu'à l'état N pour la dernière portion de la chaine\n",
    "\n",
    "Sur un exemple:\n",
    "\n",
    "```\n",
    "X0 = [ 1.  9.  8.  8.  8.  8.  8.  9.  3.  4.  5.  6.  6.  6.  7.  7.  8.  9.  0.  0.  0.  1.  1.]\n",
    "S0 = [ 0.  0.  0.  0.  0.  0.  1.  1.  1.  1.  1.  1.  2.  2.  2.  2.  2.  3.  3.  3.  3.  3.  3.]\n",
    "```\n",
    "\n",
    "Au niveau de la mise en oeuvre, vous définirez la méthode ```def initGD(X,N):```qui prend un ensemble de séquences d'observations et qui retourne l'ensemble des séquences d'états. Pour chaque séquence $x$ vous pouvez utiliser:\n",
    "```\n",
    "np.floor(np.linspace(0,N-.00000001,len(x)))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6. 5. 1. 8. 6. 0. 0. 9. 1. 1. 3. 6. 3. 4.]\n",
      "[0. 0. 0. 0. 1. 1. 1. 2. 2. 2. 3. 3. 3. 3.]\n"
     ]
    }
   ],
   "source": [
    "def initGD(x,N):\n",
    "    return np.floor(np.linspace(0,N-0.00000001,len(x)))\n",
    "    \n",
    "# 14 observations aléatoires\n",
    "x = np.floor(np.random.rand(14)*10)\n",
    "print(x)\n",
    "print(initGD(x,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 36.214493 347.719116 322.088898 312.230957 314.851013 315.487213\n",
      " 313.556702 326.534973 141.288971 167.606689 199.321594 217.911087\n",
      " 226.443298 235.002472 252.354492 270.045654 291.665161 350.934723\n",
      "  17.892815  20.281025  28.207161  43.883423  53.459026] \n",
      " [0. 0. 0. 0. 0. 1. 1. 1. 1. 2. 2. 2. 2. 2. 3. 3. 3. 3. 4. 4. 4. 4. 4.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-84f7995b9fe4>:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  Q = np.array([initGD(X[i], N) for i in range(len(X))])\n"
     ]
    }
   ],
   "source": [
    "# Construire la base Q des états, initialisés en Gauche-Droite correspondant aux observations X\n",
    "N = 5 # nombre d'états\n",
    "\n",
    "Q = np.array([initGD(X[i], N) for i in range(len(X))])\n",
    "\n",
    "print(X[0],\"\\n\",Q[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check:\n",
    "```\n",
    "[ 36.214493 347.719116 322.088898 312.230957 314.851013 315.487213\n",
    " 313.556702 326.534973 141.288971 167.606689 199.321594 217.911087\n",
    " 226.443298 235.002472 252.354492 270.045654 291.665161 350.934723\n",
    "  17.892815  20.281025  28.207161  43.883423  53.459026] \n",
    "[0. 0. 0. 0. 0. 1. 1. 1. 1. 2. 2. 2. 2. 2. 3. 3. 3. 3. 4. 4. 4. 4. 4.]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A2. Apprentissage\n",
    "\n",
    "Etant donné la structure d'un MMC:\n",
    "* les observations n'influencent pas les états: les matrices $\\Pi, A$ s'obtiennent comme dans un modèle de Markov simple \n",
    "* chaque observation ne dépend que de l'état courant\n",
    "\n",
    "La nature des données nous pousse à considérer des lois de probabilités discrètes quelconques pour les émissions. L'idée est donc de procéder par comptage en définissant la matrice $B$ comme suit:\n",
    "* $K$ colonnes (nombre d'observations), $N$ lignes (nombre d'états)\n",
    "* Chaque ligne correspond à une loi d'émission pour un état (ie, chaque ligne somme à 1)\n",
    "Ce qui donne l'algorithme:\n",
    "1. $b_{ij}$ = comptage des émissions depuis l'état $s_i$ vers l'observation $x_j$\n",
    "1. normalisation des lignes de $B$\n",
    "\n",
    "Donner le code de la fonction ```def learnHMM(allX, allS, N, K):``` qui apprend un modèle à partir d'un ensemble de couples (seq. d'observations, seq. d'états)\n",
    "\n",
    "***Variante stabilisée***: afin d'éviter les transitions à probabilité nulle et de régulariser l'ensemble du système, il est intéressant d'initialiser les matrices de transition à une valeur non nulle. Cette initialisation *spéciale* peut être faite de manière optionnelle en utilisant les arguments par défaut de python:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learnHMM(allx, allq, N, K, initTo0=False, eps=1e-5):\n",
    "    if initTo0:\n",
    "        A = np.zeros((N,N))\n",
    "        B = np.zeros((N,K))\n",
    "        Pi = np.zeros(N)\n",
    "    else:\n",
    "        A = np.zeros((N,N)) # np.ones((N,N))*eps => non pertinent pour un modèle GD\n",
    "        B = np.ones((N,K))*eps\n",
    "        Pi = np.ones(N)*eps        \n",
    "    \n",
    "    Pi[0] = 1\n",
    "        \n",
    "    for i in range(N-1):\n",
    "        for j in range(K):\n",
    "            A[allq[i][j]][allq[i+1][j]] += 1\n",
    "            B[allq[i][j]][allx[i][j]]   += 1\n",
    "    B[allq[i+1]][allx[i+1]] += 1\n",
    "    \n",
    "    A = A / np.maximum(A.sum(axis=1).reshape(N,1),axis=1)\n",
    "    B = B / np.maximum(B.sum(axis=1).reshape(N,1),axis=1)\n",
    "    \n",
    "    return Pi , A, B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-159e1e04be62>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return np.array(xd)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-71122d5dbb21>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mK\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m \u001b[1;31m# discrétisation (=10 observations possibles)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mXd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdiscretise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mPi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mB\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlearnHMM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXd\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mQ\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecimals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecimals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-38-427ae7d38166>\u001b[0m in \u001b[0;36mlearnHMM\u001b[1;34m(allx, allq, N, K, initTo0, eps)\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mallq\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m             \u001b[0mA\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mallq\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mallq\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;31m#             B[allq[i][j]][allx[i][j]]   += 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;31m#     B[allq[i+1]][allx[i+1]] += 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "# le nombre d'états a été fixé au dessus (à 5)\n",
    "K = 10 # discrétisation (=10 observations possibles)\n",
    "Xd = discretise(X,K)\n",
    "Pi, A, B = learnHMM(Xd[Y=='a'],Q[Y=='a'],N,K, True)\n",
    "\n",
    "print(Pi,\"\\n\", np.around(A,decimals=2), \"\\n\", np.around(B,decimals=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation sur les séquences de la classe 'a':\n",
    "```\n",
    "  Pi=[ 1.  0.  0.  0.  0.]\n",
    "\n",
    "  A=[[0.79 0.21 0.   0.   0.  ]\n",
    "     [0.   0.76 0.24 0.   0.  ]\n",
    "     [0.   0.   0.77 0.23 0.  ]\n",
    "     [0.   0.   0.   0.76 0.24]\n",
    "     [0.   0.   0.   0.   1.  ]] \n",
    "\n",
    "  B=[[0.06 0.02 0.   0.   0.   0.   0.   0.04 0.49 0.4 ]\n",
    "     [0.   0.04 0.   0.13 0.09 0.13 0.02 0.09 0.41 0.09]\n",
    "     [0.   0.   0.   0.02 0.12 0.5  0.31 0.04 0.   0.  ]\n",
    "     [0.07 0.   0.   0.   0.   0.   0.26 0.33 0.2  0.15]\n",
    "     [0.73 0.12 0.   0.   0.   0.   0.   0.02 0.02 0.12]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Algorithme de décodage, Viterbi\n",
    "\n",
    "Retrouver **la sequence d'état caché la plus probable** ayant permis de générer la séquence d'observation.\n",
    "\n",
    "Rappels sur l'algorithme Viterbi (1967):\n",
    "\n",
    "- Il sert à estimer la séquence d'états la plus probable étant donnés les observations et le modèle.\n",
    "- Il peut servir à approximer la probabilité de la séquence d'observation étant donné le modèle. \n",
    "\n",
    "1\\. Initialisation (avec les indices à 0 en python): \n",
    "\n",
    "$$\\begin{array}{ccccccccc} \\delta_{0} (i) &=& \\log \\pi_{i} +\\log b_{i} (x_{1}) \\\\ \\Psi_{0}(i) &=& -1 \\mbox{ Note: -1 car non utilisé normalement} \\end{array}$$\n",
    "\n",
    "2\\. Récursion: \n",
    "\n",
    "$$ \\begin{array}{ccccccccc} \\delta_{t} (j) &=&\\displaystyle \\left[\\max_{i} \\delta_{t-1}(i) + \\log a_{ij}\\right] + \\log b_{j}(x_{t}) \\\\ \\Psi_{t}(j) &=&\\displaystyle \\arg\\max_{i\\in [1,\\ N]} \\delta_{t-1} (i) + \\log a_{ij} \\end{array}$$\n",
    "\n",
    "3\\. Terminaison (indices à {$T-1$} en python) \n",
    "\n",
    "$$ S^{\\star} = \\max_{i} \\delta_{T-1}(i)$$\n",
    "\n",
    "4\\. Chemin $$\\begin{array}{ccccccccc} s_{T-1}^{\\star} & = &\\displaystyle \\arg\\max_{i} \\delta_{T-1}(i) \\\\ s_{t}^{\\star} & = & \\displaystyle \\Psi_{t+1}(s_{t+1}^{\\star}) \\end{array}$$\n",
    "\n",
    "L'estimation de $\\log p(x_0^{T-1} | \\lambda)$ est obtenue en cherchant la plus grande probabilité dans la dernière colonne de $\\delta$. Donner le code de la méthode `viterbi(x,Pi,A,B):` \n",
    "\n",
    "### B.1 Définition de la fonction viterbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(x,Pi,A,B):\n",
    "    T = len(x)\n",
    "    N = len(Pi)\n",
    "    logPI = np.log(Pi)\n",
    "    logA = np.log(A)\n",
    "    logB = np.log(B)\n",
    "    logdelta = np.zeros((N,T))\n",
    "    psi = np.zeros((N,T))\n",
    "    S = np.zeros(T) # les états à retourner\n",
    "    # votre code\n",
    "    # pass\n",
    "\n",
    "    return S, logp # états + approximation de la vraimsemblance des obs par rapport au modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "q, p = viterbi(Xd[0], Pi, A, B) # attention à bien donner la version discrétisée des observations\n",
    "\n",
    "print(Xd[0],\"\\n\",q, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check:\n",
    "```\n",
    "# décodage\n",
    "[1. 9. 8. 8. 8. 8. 8. 9. 3. 4. 5. 6. 6. 6. 7. 7. 8. 9. 0. 0. 0. 1. 1.] \n",
    " [0. 0. 0. 0. 0. 0. 0. 0. 1. 2. 2. 2. 2. 2. 3. 3. 3. 3. 4. 4. 4. 4. 4.] \n",
    "# log vraisemblance\n",
    "-38.09356554559258\n",
    "```\n",
    "\n",
    "### B2. Utilisation de l'algorithme de Viterbi pour la classification\n",
    "\n",
    "Nous aurions normalement du utiliser l'algorithme forward pour estimer la vraisemblance... Mais l'algorithme de Viterbi est plus versatile et il nous permettra de mettre en oeuvre des stratégies d'apprentissage dans les questions suivantes.\n",
    "\n",
    "Nous allons donc utiliser la vraisemblance du chemin le plus probable (dans l'espace des états) comme une approximation de la vraisemblance de l'ensemble des chemins.\n",
    "\n",
    "1. Calculer un modèle par lettre\n",
    "1. Estimer la vraisemblance des lettres dans chaque modèle\n",
    "1. Donner un score de classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcul des 26 modèles\n",
    "models = \n",
    "\n",
    "# affectation des signaux = calcul de vraisemblance[n_sig x n_modèles]\n",
    "all_proba = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = np.array(all_proba).argmax(1) # à modifier si votre all_proba est transposé\n",
    "\n",
    "traduction = {ch:i for i,ch in enumerate(np.unique(Y))}\n",
    "Ynum = np.array([traduction[y] for y in Y])\n",
    "\n",
    "tx_bonne_classif = np.where(ypred == Ynum,1.,0).mean()\n",
    "print(\"Taux de bonne classification\" , tx_bonne_classif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check : 85.07% de bonne classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B3. Séparation apprentissage / test\n",
    "\n",
    "L'expérience précédente n'est pas très satisfaisante, l'évalution des performances n'étant pas fiable.\n",
    "Nous allons donc séparer les données en un jeu d'apprentissage et un jeu de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOUT LE CODE EST FOURNI ICI\n",
    "\n",
    "# séparation stratifiée par classe (non randomisée)\n",
    "def separation_app_test(Y, pc_app):\n",
    "    ind_app  = []\n",
    "    ind_test = []\n",
    "    for cl in np.unique(Y):\n",
    "        index = np.where(Y == cl)[0]\n",
    "        ind_app += index[:int(len(index)*pc_app)].tolist()\n",
    "        ind_test += index[int(len(index)*pc_app):].tolist()\n",
    "    return ind_app, ind_test\n",
    "\n",
    "pc_app = 0.8 # pourcentage de points en apprentissage\n",
    "ind_app, ind_test = separation_app_test(Y, pc_app)\n",
    "# separation des données\n",
    "Xd_app, Xd_test = Xd[ind_app], Xd[ind_test]\n",
    "Y_app, Y_test   = Y[ind_app], Y[ind_test]\n",
    "Yn_app, Yn_test = Ynum[ind_app], Ynum[ind_test]\n",
    "Q_app, Q_test   = Q[ind_app], Q[ind_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apprentissage des modèles sur les données d'apprentissage seulement\n",
    "\n",
    "models = \n",
    "\n",
    "# calcul des affectations\n",
    "all_proba_app = \n",
    "all_proba_test = \n",
    "\n",
    "# calcul des performances\n",
    "ypred_app  = np.array(all_proba_app).argmax(1) # a verifier en fonction de vos amtrice all_proba\n",
    "ypred_test = np.array(all_proba_test).argmax(1)\n",
    "tx_bonne_classif_app = np.where(ypred_app == Yn_app,1.,0).mean()\n",
    "tx_bonne_classif_test = np.where(ypred_test == Yn_test,1.,0).mean()\n",
    "print(\"Taux de bonne classification\" , tx_bonne_classif_app, tx_bonne_classif_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check :\n",
    "\n",
    "Taux de bonne classification 0.8509615384615384 \n",
    "0.7666666666666667"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B4. Matrice de confusion\n",
    "\n",
    "Afin de mieux comprendre les erreurs, tracer la matrice de confusion associée aux lettre\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mat_conf(Y, Yhat):\n",
    "    nCl = len(np.unique(Y)) # on espere que le vecteur des Y contient toutes les valeurs entre 0 et nCl-1\n",
    "    conf = np.zeros((nCl,nCl))\n",
    "    for y,yh in zip(Y,Yhat):\n",
    "        conf[y, yh] += 1\n",
    "    return conf\n",
    "\n",
    "# appel à la fonction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B5. OPT. Trouver les paramètres de discrétisation et de nombre d'états optimaux vis à vis des données de test\n",
    "\n",
    "Cette procédrure s'appelle un grid-search: proposer des valeurs puis tester de manière exhaustive. \n",
    "Cette procédure est plutot simple à implémenter... mais longue en temps de calcul.\n",
    "\n",
    "**NE PAS FAIRE EN TP**\n",
    "\n",
    "**Note:** à force de faire des évaluations sur les données de test, on introduit un risque de sur-apprentissage de ces données."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Baum-Welch simplifié\n",
    "\n",
    "En utilisant la procédure de Baum-Welch simplifiée vue en TD et rappelée ci-dessous, proposer un code pour apprendre (ou plutot raffiner) les modèles correspondant aux 26 lettres de l'alphabet.\n",
    "\n",
    "**Baum-Welch simplifié:**\n",
    "1. Initialisation des états cachés arbitraire (eg méthode gauche-droite)\n",
    "1. Tant que ''critère de convergence'' non atteint\n",
    "    1. Apprentissage des modèles $\\lambda_{lettre}=\\{\\Pi, A, B\\}$\n",
    "    1. Estimation des états cachés par Viterbi\n",
    "\n",
    "Le critère de convergence sera la vraisemblance.\n",
    "* A chaque itération $k$ et pour toutes les lettres $lettre$, calculer pour l'ensemble des séquences d'observation : \n",
    "$$\\log\\mathcal L^k = \\sum_{lettre}\\sum_i \\log p(\\mathbf x_i^{lettre} | \\lambda_{lettre}^k)$$\n",
    "* Lorsque la vraisemblance n'évolue plus (ie $\\frac{\\log\\mathcal L^k - \\log\\mathcal L^{k+1}}{\\log\\mathcal L^k} < 1e-4$), sortir de la boucle de mise à jour.\n",
    "\n",
    "\n",
    "* Donner l'implémentation de la méthode d'apprentissage\n",
    "* Tracer la courbe de l'évolution de la vraisemblance au cours des itérations\n",
    "\n",
    "### C1. Donner le code de la fonction baumwelch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nIter : nombre d'iteration max (pour éviter les boucles infinies)\n",
    "# Xd : les données d'observations discrétisées\n",
    "# Yn : les étiquettes au format numérique\n",
    "# models: une initialisation maline des modèles à apprendre\n",
    "#     - RAPPEL: l'initialisation est critique !\n",
    "\n",
    "def baumwelch(nIter, Xd, Yn, models):\n",
    "    N = len(models[0][0])\n",
    "    K = models[0][2].shape[1] # la seconde dimension de B\n",
    "    print(N,K)\n",
    "    # Baum Welch (simplifie)\n",
    "    nCl = len(np.unique(Y))\n",
    "    nIter = 10\n",
    "    # A compléter\n",
    "    \n",
    "    return models\n",
    "\n",
    "models = baumwelch(30, Xd_app, Yn_app, models)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C2. Evaluation des performances\n",
    "\n",
    "Les performances sont-elles meilleures après optimisation du modèle?\n",
    "\n",
    "Faudrait-il refaire une procédure de grid-search basée sur l'algorithme Baum-Welch?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcul des affectations en apprentissage et en test\n",
    "all_proba_app = \n",
    "all_proba_test = \n",
    "\n",
    "# calcul des perfromances\n",
    "ypred_app  = np.array(all_proba_app).argmax(1)\n",
    "ypred_test = np.array(all_proba_test).argmax(1)\n",
    "tx_bonne_classif_app = np.where(ypred_app == Yn_app,1.,0).mean()\n",
    "tx_bonne_classif_test = np.where(ypred_test == Yn_test,1.,0).mean()\n",
    "print(\"Taux de bonne classification\" , tx_bonne_classif_app, tx_bonne_classif_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D. Génération de lettres\n",
    "\n",
    "De la même qu'un modèle de Markov, la chaine de Markov cachée est un modèle génératif: il est possible de générer des lettre.\n",
    "\n",
    "Avec la version discrétisée des angles, ça ne va toujours pas donner de résultats formidables... Mais c'est possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
